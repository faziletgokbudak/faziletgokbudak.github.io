<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>One-shot Retouching</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://faziletgokbudak.github.io/one-shot/img/architecture.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="600">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://faziletgokbudak.github.io/one-shot">
    <meta property="og:title" content="One-shot Detail Retouching with Patch Space NeuralTransformation Blending">
    <meta property="og:description" content="Our technique automatically retouches photos by learning the desired edits from one example <i>before-after pair</i> (as indicated in images or shown in small boxes). The learned retouches accurately capture edits in intricate details, such as skin textures as shown in input-retouched pairs.">

<!-- 
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta name="twitter:description" content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">
    <meta name="twitter:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg">

 -->
    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
		One-shot Detail Retouching with <br> Patch Space Neural Transformation Blending <br>
            </h2>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">


	<script
	  defer
	  src="https://unpkg.com/img-comparison-slider@7/dist/index.js"
	></script>
	<link
	  rel="stylesheet"
	  href="https://unpkg.com/img-comparison-slider@7/dist/styles.css"
	/>

	
	
        <div class="row">
            <div class="col-md-8 col-md-offset-2">

                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
				<img-comparison-slider>
				  <img slot="first" src="img/68909.png" width="100%" ; height: auto ;style="margin:auto;"/>
				  <img slot="second" src="img/PT_makeup2_test6_256_with_var.png" width="100%" ; height: auto; style="margin:auto;"/>
				</img-comparison-slider>                                
                                <canvas height=0 class="videoMerge" id="water2Merge"></canvas>
                        </td>
                        <td align="left" valign="top" width="50%">
				<img-comparison-slider>
				  <img slot="first" src="img/test1.png" width="100%" ; height: auto; style="margin:auto;"/>
				  <img slot="second" src="img/PT_sharpen_test1_256_with_var.png" width="100%" ; height: auto; style="margin:auto;"/>
				</img-comparison-slider>                                
                                <canvas height=0 class="videoMerge" id="water3Merge"></canvas>
                        </td>
                    </tr>
                </table>
		<div class="text-justify">
			Our technique learns different retouching styles from a single example before-after image pair, which can be found in Additional Results (Left: Top row, Right: Bottom row). 
		</div>

            </div>
        </div>

<!-- 	    
	    
	 
	<img-comparison-slider>
	  <img slot="first" src="img/68909.png" width="400" height="400" />
	  <img slot="second" src="img/PT_makeup2_test6_256_with_var.png" width="400" height="400" />
	</img-comparison-slider>


	<img-comparison-slider>
	  <img slot="first" src="img/test1.png" width="400" height="400" />
	  <img slot="second" src="img/PT_sharpen_test1_256_with_var.png" width="400" height="400" />
	</img-comparison-slider>
	  -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Photo retouching is a difficult task for novice users as it requires expert knowledge and advanced tools. Photographers often spend a great deal of time generating high-quality retouched photos with intricate details. In this paper, we introduce a one-shot learning based technique to automatically retouch details of an input image based on just a single pair of before and after example images. Our approach provides accurate and generalizable detail edit transfer to new images. We achieve these by proposing a new representation for image to image maps. Specifically, we propose neural field based transformation blending in the patch space for defining patch to patch transformations for each frequency band. This parametrization of the map with anchor transformations and associated weights, and spatio-spectral localized patches, allows us to capture details well while staying generalizable. We evaluate our technique both on known ground truth filters and artist retouching edits. Our method accurately transfers complex detail retouching edits.
	    </div>
        </div>
<!-- 	    max-height: 450px; -->
	    
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <image src="img/architecture.png" class="img-responsive" alt="overview" style="margin:auto;">
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Transformation Blending
                </h3>
                <div class="text-justify">
<!-- 			<i>Integrated Directional Encoding</i> -->
		    A novel learning-based image mapping technique in patch space designed to capture highly detailed retouching styles. Inspired by artistic retouching pipelines, our approach provides accurate representations of intricate details while preserving the semantics of the input image. With the help of prior knowledge about artistic retouching pipelines, our map representation can generalize well to new images with different structures.                </div>
            </div>
        </div>
		
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Patch-adaptive Weights
                </h3>
                <div class="text-justify">
		   Transformations are tuned based on patch values and hence achieve local adjustments. Weights learned by the MLP block blend various transformation matrices. Our approach helps us mimic local edit tools of professionals, such as a brush tool or local filters.                 </div>
        </div>
		
<!--         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                   Nuances of Different Retouching Styles
                </h3>
        	<image src="img/RetouchingStyles_cropped-1.png" class="img-responsive" alt="overview" style="margin:auto;">
	    </div>
        </div> -->
		

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Additional Results
                </h3>
	
<!-- 		<image src="img/AdditionalResults-1.png" class="img-responsive" alt="overview"  style="margin:auto;">
 -->
            </div>
        </div>
<!-- 	<div class="text-justify">
		Our technique automatically retouches photos by learning the desired edits from one example <i>before-after pair</i> (as indicated in images or shown in small boxes). The learned retouches accurately capture edits in intricate details, such as skin textures as shown in input-retouched pairs.
 -->
<!-- 
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Additional Results
                </h3>
                <div class="text-justify">
            </div>
        </div> -->
		
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    
                </h3>
        	<image src="img/AdditionalResults2.png" class="img-responsive" alt="overview"  style="margin:auto;">
	                <div class="text-justify">
			    Our technique automatically retouches photos by learning the desired edits from one example <i>before-after pair</i> (as indicated in images or shown in small boxes). The learned retouches accurately capture edits in intricate details, such as skin textures as shown in input-retouched pairs.
			</div>
	    </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>

    
<!--         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
		</div>
	    </div>
	</div>

</body></html> -->
